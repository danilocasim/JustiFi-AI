<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Voice Legal Consultation</title>

    <script src="https://download.agora.io/sdk/release/AgoraRTC_N-4.21.0.js"></script>

    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="transcript.css" />
    <style>
      .listening-indicator {
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
        opacity: 0;
        height: 0;
        overflow: hidden;
        transition: opacity 0.3s ease;
      }

      .listening-indicator.show {
        opacity: 1;
        height: auto;
      }

      .voice-wave {
        display: flex;
        align-items: flex-end;
        gap: 4px;
        height: 28px;
        margin-bottom: 4px;
      }

      .voice-wave span {
        width: 4px;
        height: 6px;
        background: #3b82f6;
        border-radius: 4px;
        display: inline-block;
        animation: wave 1s infinite ease-in-out;
      }

      .voice-wave span:nth-child(1) {
        animation-delay: 0s;
      }
      .voice-wave span:nth-child(2) {
        animation-delay: 0.1s;
      }
      .voice-wave span:nth-child(3) {
        animation-delay: 0.2s;
      }
      .voice-wave span:nth-child(4) {
        animation-delay: 0.3s;
      }
      .voice-wave span:nth-child(5) {
        animation-delay: 0.4s;
      }

      @keyframes wave {
        0%,
        100% {
          height: 6px;
        }
        50% {
          height: 28px;
        }
      }
    </style>
  </head>

  <body>
    <%- include("partials/navbar") %>
    <div class="bg-gray-50 p-5 ">
      <div class="call-container mt-5 max-w-7xl mx-0 lg:mx-40 px-4">
        <div class="flex flex-col lg:flex-row lg:space-x-6">
          <!-- Main Panel -->
          <div class="flex-1 bg-white rounded-xl shadow-sm p-6">
            <!-- Header -->
            <div class="mb-6">
              <h1 class="text-3xl font-bold text-gray-800 mb-3">Live Call</h1>
              <div class="flex items-center gap-3">
                <div class="status">
                  <span
                    class="inline-flex items-center mb-3 gap-2 bg-green-500 text-white px-3 py-1 rounded-full text-sm font-medium"
                  >
                    <span class="w-2 h-2 bg-white rounded-full"></span>
                    <div class="status-dot" id="statusDot"></div>
                    <span id="callStatus">Connecting to Agora...</span>
                  </span>
                  <div class="category-badge text-gray-600 font-medium">
                    <%= session.category %>
                  </div>
                </div>
              </div>
            </div>

            <!-- Call Status Card -->
            <div class="bg-white rounded-lg shadow-sm p-12 mb-6 text-center">
              <div class="flex flex-col items-center">
                <div
                  class="w-24 h-24 bg-blue-500 rounded-full flex items-center justify-center mb-4"
                >
                  <svg class="w-12 h-12 text-white" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 5a2 2 0 012-2h3.28a1 1 0 01.948.684l1.498 4.493a1 1 0 01-.502 1.21l-2.257 1.13a11.042 11.042 0 005.516 5.516l1.13-2.257a1 1 0 011.21-.502l4.493 1.498a1 1 0 01.684.949V19a2 2 0 01-2 2h-1C9.716 21 3 14.284 3 6V5z"></path>
                  </svg>
                </div>
                <h2 class="text-2xl font-bold text-gray-800 mb-2">
                  JustiFi AI
                </h2>
                <div class="listening-indicator" id="listeningIndicator">
                  <div class="voice-wave">
                    <span></span><span></span><span></span><span></span
                    ><span></span>
                  </div>
                  <p>Listening to you...</p>
                </div>

                <!-- Controls -->
                <div class="flex flex-row items-center mt-4">
                  <button class="control-btn mic-btn muted bg-yellow-500 hover:bg-yellow-600 
text-white font-medium flex items-center mb-2 lg:mb-0 gap-2 transition-colors rounded-full

px-4 py-2 text-sm
sm:px-5 sm:py-3 sm:text-base  
md:px-6 md:py-3 md:text-lg  
lg:px-7 lg:py-3 lg:text-base

mr-3 sm:mr-4 md:mr-5"

                    id="micBtn"
                    onclick="toggleMic()"
                    disabled
                  >
                    <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewBox="0 -960 960 960" width="24px" fill="#ffffffff"><path d="m710-362-58-58q14-23 21-48t7-52h80q0 44-13 83.5T710-362ZM480-594Zm112 112-72-72v-206q0-17-11.5-28.5T480-800q-17 0-28.5 11.5T440-760v126l-80-80v-46q0-50 35-85t85-35q50 0 85 35t35 85v240q0 11-2.5 20t-5.5 18ZM440-120v-123q-104-14-172-93t-68-184h80q0 83 57.5 141.5T480-320q34 0 64.5-10.5T600-360l57 57q-29 23-63.5 39T520-243v123h-80Zm352 64L56-792l56-56 736 736-56 56Z"/></svg>
                  </button>
                  <button
                   class="control-btn end-btn
    bg-red-500 hover:bg-red-600 text-white font-medium
    flex items-center gap-2 transition-colors rounded-full

    px-4 py-2 text-sm        
    sm:px-5 sm:py-3 sm:text-base
    md:px-6 md:py-3 md:text-lg
    lg:px-6 lg:py-3 lg:text-base

    w-full sm:w-auto
  "
                    onclick="endCall()"
                  >
                    <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewBox="0 -960 960 960" width="24px" fill="#ffffffff"><path d="m136-304-92-90q-12-12-12-28t12-28q88-95 203-142.5T480-640q118 0 232.5 47.5T916-450q12 12 12 28t-12 28l-92 90q-11 11-25.5 12t-26.5-8l-116-88q-8-6-12-14t-4-18v-114q-38-12-78-19t-82-7q-42 0-82 7t-78 19v114q0 10-4 18t-12 14l-116 88q-12 9-26.5 8T136-304Zm104-198q-29 15-56 34.5T128-424l40 40 72-56v-62Zm480 2v60l72 56 40-38q-29-26-56-45t-56-33Zm-480-2Zm480 2Z"/></svg>
                  </button>
                </div>
              </div>
            </div>

            <!-- Live Transcript -->
            <div class="bg-[#F1F5F9] rounded-xl p-6 min-h-[400px]">
              <h3 class="text-xl font-bold text-gray-800 mb-1">
                Live Transcript
              </h3>
              <p class="text-gray-500 text-sm mb-6">Real-time conversation</p>
              <div class="space-y-4 max-w-xl mx-auto">
                <div
                  class="transcript-box flex flex-col items-start space-x-2"
                  id="transcript"
                ></div>
              </div>
            </div>
          </div>

          <!-- Sidebar -->
          <div
            class="w-full lg:w-1/4 bg-white rounded-xl shadow-sm p-6 mt-6 lg:mt-0"
          >
            <h3 class="text-lg font-bold mb-3">üìã Action Steps</h3>
            <ul class="space-y-2">
              <li class="flex items-center gap-2">
                <input type="checkbox" class="w-4 h-4" /> Connecting to voice
                service...
              </li>
            </ul>
            <h3 class="text-lg font-bold mt-6 mb-2">üìû Important Contacts</h3>
            <div class="text-sm space-y-1">
              <p><strong>PAO:</strong> 929-9436</p>
              <p><strong>DOLE:</strong> 1349</p>
              <p><strong>Barangay:</strong> Local directory</p>
            </div>
          </div>
        </div>
      </div>
    </div>

    <script>
      // ===== GLOBAL VARIABLES =====
      const sessionId = "<%= session.sessionId %>";
      const channelName = "legal-buddy-" + sessionId.substring(0, 8);
      let rtcClient = null;
      let localAudioTrack = null;
      let isMuted = true;
      let isAISpeaking = false;
      let isRecognizing = false;
      let conversationHistory = [];
      let isProcessing = false;
      const micBtn = document.getElementById("micBtn");
      const statusDot = document.getElementById("statusDot");
      const callStatus = document.getElementById("callStatus");
      const listeningIndicator = document.getElementById("listeningIndicator");
      const synth = window.speechSynthesis;
      // ===== SPEECH RECOGNITION SETUP =====
      const SpeechRecognition =
        window.SpeechRecognition || window.webkitSpeechRecognition;
      if (!SpeechRecognition) {
        alert(
          "‚ö†Ô∏è Your browser doesn't support speech recognition. Please use Chrome, Edge, or Safari."
        );
      }
      const recognition = new SpeechRecognition();
      recognition.lang = "en-US";
      recognition.continuous = false;
      recognition.interimResults = false;
      recognition.maxAlternatives = 1;
      recognition.onstart = () => {
        console.log("üé§ Speech recognition STARTED");
        isRecognizing = true;
      };
      recognition.onresult = async (event) => {
        console.log("üìù Speech recognition got result");
        const transcript = event.results[0][0].transcript;
        const confidence = event.results[0][0].confidence;
        console.log(`‚úÖ You said: "${transcript}" (confidence: ${confidence})`);
        stopListening();
        callStatus.textContent = "Processing your message...";
        addTranscript("USER", transcript);
        await saveTranscript("USER", transcript);
        await getAIResponseAndSpeak(transcript);
      };
      recognition.onerror = (event) => {
        console.error("‚ùå Speech recognition error:", event.error);
        let errorMessage = "";
        switch (event.error) {
          case "no-speech":
            errorMessage = "No speech detected. Please try again.";
            break;
          case "audio-capture":
            errorMessage =
              "Microphone not accessible. Please check permissions.";
            break;
          case "not-allowed":
            errorMessage =
              "Microphone access denied. Please allow microphone access in browser settings.";
            break;
          case "network":
            errorMessage = "Network error. Please check your connection.";
            break;
          case "aborted":
            console.log("Recognition aborted (normal if user stopped)");
            return;
          default:
            errorMessage = `Speech recognition error: ${event.error}`;
        }
        if (errorMessage) {
          callStatus.textContent = errorMessage;
          alert(errorMessage);
        }
        resetMicState();
      };
      recognition.onend = () => {
        console.log("üî¥ Speech recognition ENDED");
        isRecognizing = false;
        if (!isAISpeaking && isMuted) {
          callStatus.textContent = "Ready to talk";
        }
      };
      // ===== AGORA INITIALIZATION =====
      async function initializeCall() {
        try {
          console.log("üöÄ Initializing Agora call...");
          const tokenResponse = await fetch("/api/agora/token", {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
            },
            body: JSON.stringify({
              channelName,
              uid: 0,
            }),
          });
          const tokenData = await tokenResponse.json();
          console.log("‚úÖ Got Agora token");
          rtcClient = AgoraRTC.createClient({
            mode: "rtc",
            codec: "vp8",
          });
          await rtcClient.join(
            tokenData.appId,
            channelName,
            tokenData.rtcToken,
            0
          );
          console.log("‚úÖ Joined Agora channel");
          localAudioTrack = await AgoraRTC.createMicrophoneAudioTrack();
          await localAudioTrack.setMuted(true);
          await rtcClient.publish([localAudioTrack]);
          console.log("‚úÖ Published audio track");
          callStatus.textContent = "Connected - Ready to talk";
          micBtn.disabled = false;
          statusDot.classList.add("listening");
          setTimeout(() => {
            const greeting = `Hello! I'm JustiFi AI for <%= session.category %> issues. Please tell me what happened.`;
            addTranscript("AI", greeting);
            speak(greeting);
          }, 1000);
          console.log("‚úÖ Agora call initialized successfully");
        } catch (error) {
          console.error("‚ùå Agora initialization error:", error);
          callStatus.textContent = "Connection failed - Check console";
          alert(
            "Failed to connect to voice service. Please check your microphone permissions and try again."
          );
        }
      }
      // ===== MIC CONTROLS =====
      async function toggleMic() {
        if (isAISpeaking) return;

        if (isMuted) {
          try {
            await localAudioTrack.setMuted(false);
            isMuted = false;
            isRecognizing = true;

            micBtn.classList.remove("muted");
            micBtn.classList.add("listening");
            micBtn.textContent = "üé§ Listening...";

            statusDot.classList.add("listening");
            callStatus.textContent = "Listening to you - Speak now!";

            listeningIndicator.classList.add("show");

            recognition.start();
          } catch (error) {
            console.error("‚ùå Failed to start recognition:", error);
            alert("Failed to start speech recognition. Please try again.");
            resetMicState();
          }
        } else {
          stopListening();
        }
      }

      function stopListening() {
        if (isRecognizing) recognition.stop();
        isRecognizing = false;

        if (!isMuted) localAudioTrack.setMuted(true);
        isMuted = true;

        micBtn.classList.add("muted");
        micBtn.classList.remove("listening");
        micBtn.textContent = "üîá Microphone Off";

        statusDot.classList.remove("listening");
        callStatus.textContent = "Ready to talk";

        listeningIndicator.classList.remove("show");
      }

      function resetMicState() {
        isRecognizing = false;
        isMuted = true;
        isAISpeaking = false;
        if (localAudioTrack) {
          localAudioTrack.setMuted(true);
        }
        micBtn.classList.add("muted");
        micBtn.classList.remove("listening", "speaking");
        micBtn.disabled = false;
        micBtn.textContent = "üîá Microphone Off";
        statusDot.classList.remove("listening", "speaking");
        callStatus.textContent = "Ready to talk";
        listeningIndicator.classList.remove("show");
      }
      // ===== AI RESPONSE =====
      async function getAIResponseAndSpeak(userMessage) {
        isAISpeaking = true;
        micBtn.disabled = true;
        micBtn.classList.add("speaking");
        micBtn.textContent = "ü§ñ AI is thinking...";
        statusDot.classList.remove("listening");
        statusDot.classList.add("speaking");
        callStatus.textContent = "AI is responding...";
        try {
          const context = `Legal Category: <%= session.category %>. Conversation: ${conversationHistory
            .slice(-5)
            .map((t) => `${t.speaker}: ${t.text}`)
            .join(" | ")}`;
          console.log("üì§ Sending to AI:", userMessage);
          const response = await fetch("/api/ai/chat", {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
            },
            body: JSON.stringify({
              message: userMessage,
              context,
            }),
          });
          if (!response.ok) {
            throw new Error(`HTTP ${response.status}: ${response.statusText}`);
          }
          const data = await response.json();
          const aiMessage =
            data.response ||
            "Sorry, I did not understand. Can you please repeat?";
          console.log("üì• AI Response:", aiMessage);
          addTranscript("AI", aiMessage);
          await saveTranscript("AI", aiMessage);
          speak(aiMessage);
          if (conversationHistory.length >= 6 && !isProcessing) {
            processWithAI();
          }
        } catch (error) {
          console.error("‚ùå AI Chat Error:", error);
          const errorMsg = "Sorry, there was an error. Please try again.";
          addTranscript("AI", errorMsg);
          speak(errorMsg);
        }
      }
      // ===== TEXT-TO-SPEECH =====
      function speak(text) {
        synth.cancel();
        const utterance = new SpeechSynthesisUtterance(text);
        utterance.lang = "en-US";
        utterance.rate = 1.0;
        utterance.pitch = 1.0;
        const voices = synth.getVoices();
        const preferredVoice =
          voices.find(
            (v) => v.lang.startsWith("en") && v.name.includes("Female")
          ) || voices[0];
        if (preferredVoice) {
          utterance.voice = preferredVoice;
        }
        utterance.onstart = () => {
          console.log("üîä AI started speaking");
          isAISpeaking = true;
          micBtn.textContent = "üîä AI is speaking...";
          callStatus.textContent = "AI is speaking...";
        };
        utterance.onend = () => {
          console.log("‚úÖ AI finished speaking");
          isAISpeaking = false;
          micBtn.disabled = false;
          micBtn.classList.remove("speaking");
          micBtn.textContent = "üîá Microphone Off";
          statusDot.classList.remove("speaking");
          statusDot.classList.add("listening");
          callStatus.textContent = "Your turn - Click mic to speak";
        };
        utterance.onerror = (event) => {
          console.error("‚ùå Speech synthesis error:", event);
          isAISpeaking = false;
          micBtn.disabled = false;
          micBtn.classList.remove("speaking");
          callStatus.textContent = "Speech error - Ready to continue";
        };
        console.log("üó£Ô∏è Speaking:", text);
        synth.speak(utterance);
      }

      //     ===== TRANSCRIPT FUNCTIONS =====
      function addTranscript(speaker, text) {
        const transcriptBox = document.getElementById("transcript");

        // Clear placeholder text
        if (transcriptBox.querySelector('p[style*="opacity: 0.6"]')) {
          transcriptBox.innerHTML = "";
        }

        const item = document.createElement("div");
        item.className = `
  transcript-item 
  flex 
  w-full 
  mb-2
  ${speaker === "USER" ? "justify-end" : "justify-start"}
`;

        item.innerHTML = `
  <div class="flex flex-col max-w-xs">
    <div class="speaker-label text-xs font-semibold mb-1 ${
      speaker === "USER" ? "text-right" : "text-left"
    }">
      ${speaker === "USER" ? "You" : "JustiFi AI"}
    </div>

    <div class="${
      speaker === "USER"
        ? "bg-blue-500 text-white"
        : "bg-gray-200 text-gray-800"
    } p-3 rounded-lg text-sm">
      ${text}
    </div>
  </div>
`;

        transcriptBox.appendChild(item);
        transcriptBox.scrollTop = transcriptBox.scrollHeight;

        conversationHistory.push({
          speaker,
          text,
        });
      }

      async function saveTranscript(speaker, text) {
        try {
          await fetch("/api/transcript/save", {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
            },
            body: JSON.stringify({
              sessionId,
              speaker,
              text,
            }),
          });
        } catch (error) {
          console.error("Failed to save transcript:", error);
        }
      }
      // ===== SIDEBAR UPDATES =====
      async function processWithAI() {
        isProcessing = true;
        const stepsList = document.getElementById("stepsList");
        stepsList.innerHTML =
          '<li class="step-item">Analyzing conversation...</li>';
        const fullTranscript = conversationHistory
          .map((t) => `${t.speaker}: ${t.text}`)
          .join("\n");
        try {
          const response = await fetch("/api/ai/process", {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
            },
            body: JSON.stringify({
              sessionId,
              fullTranscript,
              category: "<%= session.category %>",
            }),
          });
          const summary = await response.json();
          updateActionSteps(summary);
          updateContacts(summary);
        } catch (error) {
          console.error("AI processing error:", error);
          stepsList.innerHTML =
            '<li class="step-item">Continue conversation for guidance...</li>';
        }
      }

      function updateActionSteps(summary) {
        const stepsList = document.getElementById("stepsList");
        stepsList.innerHTML = "";
        const steps = summary.recommendedSteps || [
          "Continue talking to get personalized steps",
        ];
        steps.forEach((step) => {
          const li = document.createElement("li");
          li.className = "step-item";
          li.innerHTML = `<input type="checkbox" class="step-checkbox" onchange="this.parentElement.classList.toggle('completed')">${step}`;
          stepsList.appendChild(li);
        });
      }

      function updateContacts(summary) {
        const contactsDiv = document.getElementById("contacts");
        let html = "";
        const contacts = summary.contacts || {
          PAO: "929-9436",
          DOLE: "1349",
          Barangay: "Local directory",
        };
        for (const [key, value] of Object.entries(contacts)) {
          html += `<p><strong>${key.replace("_", " ")}:</strong> ${value}</p>`;
        }
        contactsDiv.innerHTML = html;
      }
      // ===== END CALL WITH SUMMARIZATION =====
      async function endCall() {
        console.log("üìû Ending call and generating comprehensive summary...");
        // Disable button to prevent multiple clicks
        const endBtn = event.target;
        endBtn.disabled = true;
        endBtn.textContent = "‚è≥ Processing...";
        // Stop all audio/recognition
        synth.cancel();
        if (isRecognizing) {
          recognition.stop();
        }
        if (localAudioTrack) localAudioTrack.close();
        if (rtcClient) await rtcClient.leave();
        // Show loading state
        callStatus.textContent =
          "Generating your comprehensive consultation summary...";
        micBtn.disabled = true;
        micBtn.textContent = "‚è≥ Processing...";
        try {
          // First, end the session
          await fetch("/api/session/end", {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
            },
            body: JSON.stringify({
              sessionId,
            }),
          });
          // Then generate final comprehensive summary and WAIT for it
          console.log("üìã Calling final summarization endpoint...");
          callStatus.textContent = "AI is analyzing your conversation...";
          const summaryResponse = await fetch("/api/session/summarize", {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
            },
            body: JSON.stringify({
              sessionId,
            }),
          });
          if (!summaryResponse.ok) {
            const errorText = await summaryResponse.text();
            console.error("Summarization error response:", errorText);
            throw new Error(`Summarization failed: ${summaryResponse.status}`);
          }
          const summaryData = await summaryResponse.json();
          console.log("‚úÖ Comprehensive summary generated:", summaryData);
          callStatus.textContent = "Summary complete! Redirecting...";
          // Small delay to show success message
          await new Promise((resolve) => setTimeout(resolve, 1000));
          // Now redirect to summary page
          window.location.href = `/summary/${sessionId}`;
        } catch (error) {
          console.error("‚ùå Summarization error:", error);
          callStatus.textContent = "Error generating summary";
          // Show error but still allow redirect
          const shouldContinue = confirm(
            "Failed to generate comprehensive summary. Would you like to see what was captured anyway?"
          );
          if (shouldContinue) {
            window.location.href = `/summary/${sessionId}`;
          } else {
            // Re-enable button
            endBtn.disabled = false;
            endBtn.textContent = "üìû End Call & Generate Report";
            micBtn.disabled = false;
            micBtn.textContent = "üîá Microphone Off";
            callStatus.textContent = "Ready to continue or try ending again";
          }
        }
      }
      // ===== LOAD VOICES =====
      if (synth.onvoiceschanged !== undefined) {
        synth.onvoiceschanged = () => {
          console.log("Voices loaded:", synth.getVoices().length);
        };
      }
      // ===== START =====
      window.addEventListener("load", () => {
        initializeCall();
      });
    </script>
  </body>
</html>
